#!/usr/bin/env python3
"""
Integration tests for Adaptus MCP workflow
Generated by Adaptus MCP Server
"""

import os
import sys

import pytest

# Add src to path for imports
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", "src"))

try:
    from adaptus.server import analyze_debt, generate_tests, score_debt, summarize
except ImportError as e:
    pytest.skip(f"Cannot import server module: {e}", allow_module_level=True)


class TestMcpWorkflow:
    """Integration tests for complete MCP workflow"""

    def test_complete_analysis_workflow(self, tmp_path):
        """Test complete code analysis workflow"""
        # Create test project structure
        (tmp_path / "main.py").write_text(
            """
def calculate_total(items):
    total = 0
    for item in items:
        if item['price'] > 0:
            total += item['price'] * item['quantity']
    return total

class ShoppingCart:
    def __init__(self):
        self.items = []

    def add_item(self, name, price, quantity):
        self.items.append({'name': name, 'price': price, 'quantity': quantity})

    def get_total(self):
        return calculate_total(self.items)
"""
        )

        (tmp_path / "utils.py").write_text(
            """
def format_currency(amount):
    return f"${amount:.2f}"

def validate_item(item):
    required_keys = ['name', 'price', 'quantity']
    for key in required_keys:
        if key not in item:
            raise ValueError(f"Missing required key: {key}")
    return True
"""
        )

        # Step 1: Analyze debt
        analysis_result = analyze_debt(
            [str(tmp_path / "main.py"), str(tmp_path / "utils.py")]
        )

        assert len(analysis_result) == 2
        for _, metrics in analysis_result.items():
            assert "score" in metrics
            assert "loc" in metrics
            assert metrics["score"] >= 0

        # Step 2: Score individual metrics
        main_metrics = analysis_result[str(tmp_path / "main.py")]
        score_result = score_debt(main_metrics)

        assert "score" in score_result
        assert "severity" in score_result
        assert score_result["severity"] in ["low", "medium", "high"]

        # Step 3: Generate tests for main module
        test_result = generate_tests("main", "pytest", "unit")

        assert "template" in test_result
        assert "class TestMain" in test_result["template"]
        assert "import pytest" in test_result["template"]

        # Step 4: Summarize repository
        summary_result = summarize(str(tmp_path))

        assert summary_result["summary"]["total_files"] == 2
        assert len(summary_result["file_analysis"]) == 2
        assert "recommendations" in summary_result

    def test_semantic_analysis_integration(self, tmp_path):
        """Test semantic analysis integration"""
        # Create code with various issues
        problematic_code = """
def very_long_function_to_refactor(p1, p2, p3, p4, p5, p6, p7, p8):
    result = []
    for item in range(100):
        if item % 2 == 0:
            if item > 50:
                if item < 75:
                    result.append(item * 2)
                else:
                    result.append(item * 3)
            else:
                result.append(item)
    return result

class VeryLargeClassWithManyMethods:
    def method1(self): pass
    def method2(self): pass
    def method3(self): pass
    def method4(self): pass
    def method5(self): pass
    def method6(self): pass
    def method7(self): pass
    def method8(self): pass
    def method9(self): pass
    def method10(self): pass
    def method11(self): pass
    def method12(self): pass
    def method13(self): pass
    def method14(self): pass
    def method15(self): pass
    def method16(self): pass
    def method17(self): pass
    def method18(self): pass
    def method19(self): pass
    def method20(self): pass
    def method21(self): pass
    def method22(self): pass
    def method23(self): pass
"""

        test_file = tmp_path / "problematic.py"
        test_file.write_text(problematic_code)

        # Analyze with semantic analysis
        result = analyze_debt([str(test_file)], include_semantic=True)

        metrics = result[str(test_file)]
        assert "semantic_analysis" in metrics

        semantic = metrics["semantic_analysis"]
        assert semantic["summary"]["total_issues"] > 0

        # Check that score is adjusted for semantic issues
        assert metrics["score"] > _score_formula(
            {
                k: v
                for k, v in metrics.items()
                if k not in ["semantic_analysis", "score"]
            }
        )

    def test_framework_specific_test_generation(self, tmp_path):
        """Test framework-specific test generation"""
        # Create a simple module
        (tmp_path / "calculator.py").write_text(
            """
class Calculator:
    def add(self, a, b):
        return a + b

    def subtract(self, a, b):
        return a - b

    def multiply(self, a, b):
        return a * b

    def divide(self, a, b):
        if b == 0:
            raise ValueError("Cannot divide by zero")
        return a / b
"""
        )

        # Generate pytest tests
        pytest_result = generate_tests("calculator", "pytest", "unit")
        assert "pytest.fixture" in pytest_result["template"]
        assert "@pytest.mark.parametrize" in pytest_result["template"]

        # Generate unittest tests
        unittest_result = generate_tests("calculator", "unittest", "unit")
        assert "unittest.TestCase" in unittest_result["template"]
        assert "setUp" in unittest_result["template"]

        # Generate integration tests
        integration_result = generate_tests("calculator", "pytest", "integration")
        assert "Integration" in integration_result["template"]
        assert "setup_integration" in integration_result["template"]

    def test_repository_analysis_with_hotspots(self, tmp_path):
        """Test repository analysis with hotspot detection"""
        # Create files with different complexity levels
        simple_file = tmp_path / "simple.py"
        simple_file.write_text("def hello(): return 'world'")

        complex_file = tmp_path / "complex.py"
        # Create a file with high complexity (many branches)
        complex_code = "def complex_function(x):\n"
        for i in range(50):
            complex_code += f"    if x == {i}:\n        return {i}\n"
        complex_code += "    return -1\n"

        complex_file.write_text(complex_code)

        summary = summarize(str(tmp_path))

        assert summary["summary"]["total_files"] == 2
        assert summary["summary"]["hotspots_count"] >= 1

        # Check that complex file is identified as hotspot
        hotspots = summary["hotspots"]
        complex_hotspot = next((h for h in hotspots if h["path"] == "complex.py"), None)
        assert complex_hotspot is not None
        assert complex_hotspot["score"] > 5.0 or complex_hotspot["loc"] > 200

    def test_error_handling_workflow(self):
        """Test error handling in workflow"""
        # Test with non-existent files
        result = analyze_debt(["/non/existent/file.py"])
        assert "/non/existent/file.py" in result
        assert result["/non/existent/file.py"]["error"] == 1.0

        # Test with invalid repository path
        summary = summarize("/non/existent/repo")
        assert "error" in summary
        assert "Repository path not found" in summary["error"]

        # Test with empty metrics for scoring
        empty_result = score_debt({})
        assert "score" in empty_result


class TestResourceIntegration:
    """Test resource integration with workflow"""

    def test_resource_versioning(self):
        """Test resource versioning functionality"""
        # This would test versioned resources if they existed
        # For now, test that version parameter is handled
        from adaptus.server import _read_resource

        result = _read_resource("guidelines.md", "v1.0")
        assert "Version: latest" in result or "Resource not found" in result

    def test_template_validation(self):
        """Test template validation"""
        from adaptus.server import _validate_template

        valid_template = """
## Context
This is a test template

## Analysis Framework
Some framework details

## Output Format
Output format here
"""

        result = _validate_template(valid_template)
        assert result["is_valid"] is True
        assert len(result["issues"]) == 0

        invalid_template = """
## Context
Missing required sections
"""

        result = _validate_template(invalid_template)
        assert result["is_valid"] is False
        assert len(result["issues"]) > 0


def _score_formula(metrics):
    """Helper function for testing (copied from server)"""
    mi_n = min(max((100.0 - metrics.get("mi", 0.0)) / 100.0, 0.0), 1.0)
    cc_n = min(max((metrics.get("branches", 0.0) - 10.0) / 40.0, 0.0), 1.0)
    dup_n = min(max(metrics.get("dup_rate", 0.0), 0.0), 1.0)
    td_n = min(max(metrics.get("td_ratio", 0.0), 0.0), 1.0)
    base = 0.35 * mi_n + 0.25 * cc_n + 0.20 * dup_n + 0.20 * td_n
    return round(base * 10.0, 2)


if __name__ == "__main__":
    pytest.main([__file__])
