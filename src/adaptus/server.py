#!/usr/bin/env python3
"""
Adaptus MCPサーバー
コード品質分析と設計改善を提供するMCPサーバー
"""

from __future__ import annotations

import ast
import json
import subprocess
from pathlib import Path

from mcp.server.fastmcp import FastMCP

mcp = FastMCP("Adaptus", json_response=True)


def _read_code(path: Path) -> str:
    """ファイル読込の内部関数"""
    return path.read_text(encoding="utf-8", errors="ignore")


def _read_resource(resource_path: str) -> str:
    """リソースファイル読み込みの内部関数"""
    base_path = Path(__file__).parent / "resources"
    resource_file = base_path / resource_path
    if resource_file.exists():
        return resource_file.read_text(encoding="utf-8")
    return f"Resource not found: {resource_path}"


def _basic_metrics(code: str) -> dict[str, float]:
    """Pythonコードの簡易指標を抽出"""
    try:
        tree = ast.parse(code)
    except SyntaxError:
        return {
            "loc": len(code.splitlines()),
            "funcs": 0,
            "branches": 0,
            "avg_args": 0.0,
        }

    funcs, branches, args = 0, 0, []
    for node in ast.walk(tree):
        if isinstance(node, ast.FunctionDef):
            funcs += 1
            args.append(len([a for a in node.args.args if a.arg != "self"]))
        if isinstance(node, (ast.If, ast.For, ast.While, ast.Try, ast.BoolOp)):
            branches += 1

    loc = len(
        [
            line
            for line in code.splitlines()
            if line.strip() and not line.strip().startswith("#")
        ]
    )
    avg_args = sum(args) / len(args) if args else 0.0
    return {
        "loc": float(loc),
        "funcs": float(funcs),
        "branches": float(branches),
        "avg_args": float(avg_args),
    }


def _score_formula(metrics: dict[str, float]) -> float:
    """負債スコアの計算式"""
    mi_n = min(max((100.0 - metrics.get("mi", 0.0)) / 100.0, 0.0), 1.0)
    cc_n = min(max((metrics.get("branches", 0.0) - 10.0) / 40.0, 0.0), 1.0)
    dup_n = min(max(metrics.get("dup_rate", 0.0), 0.0), 1.0)
    td_n = min(max(metrics.get("td_ratio", 0.0), 0.0), 1.0)
    base = 0.35 * mi_n + 0.25 * cc_n + 0.20 * dup_n + 0.20 * td_n
    return round(base * 10.0, 2)


@mcp.resource("adaptus://score/formula")
def get_formula() -> str:
    """スコア式のJSONを返す"""
    payload = {
        "weights": {"mi": 0.35, "cc": 0.25, "dup": 0.20, "td": 0.20},
        "note": "SQALE・SIGと定期較正すること",
    }
    return json.dumps(payload, ensure_ascii=False)


@mcp.resource("adaptus://guidelines/latest")
def get_guidelines() -> str:
    """設計規約集を返す"""
    return _read_resource("guidelines.md")


@mcp.resource("adaptus://prompt/core")
def get_core_prompts() -> str:
    """コアプロンプト雛形を返す"""
    return _read_resource("prompts.md")


@mcp.tool()
def analyze_debt(paths: list[str], lang: str = "python") -> dict[str, dict[str, float]]:
    """負債候補の分析を行う"""
    results: dict[str, dict[str, float]] = {}
    for p in paths:
        path = Path(p)
        if not path.exists():
            results[p] = {"error": 1.0}
            continue
        code = _read_code(path)
        m = (
            _basic_metrics(code)
            if lang == "python"
            else {"loc": float(len(code.splitlines()))}
        )
        m["score"] = _score_formula(m)
        results[p] = m
    return results


@mcp.tool()
def score_debt(metrics: dict[str, float]) -> dict[str, float]:
    """独自式でファイル別スコアを算出"""
    score = _score_formula(metrics)
    breakdown = {
        "mi_normalized": min(
            max((100.0 - metrics.get("mi", 0.0)) / 100.0, 0.0), 1.0
        ),
        "cc_normalized": min(
            max((metrics.get("branches", 0.0) - 10.0) / 40.0, 0.0), 1.0
        ),
        "dup_normalized": min(max(metrics.get("dup_rate", 0.0), 0.0), 1.0),
        "td_normalized": min(max(metrics.get("td_ratio", 0.0), 0.0), 1.0),
    }
    return {
        "score": score,
        "breakdown": breakdown,
        "weights": {"mi": 0.35, "cc": 0.25, "dup": 0.20, "td": 0.20},
        "severity": "low"
        if score < 3.0
        else "medium"
        if score < 7.0
        else "high",
    }


@mcp.tool()
def generate_tests(
    target: str, framework: str = "pytest"
) -> dict[str, str]:
    """テスト雛形を生成し補完指示を付与"""
    if framework == "pytest":
        test_template = f'''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Test suite for {target}
Generated by Adaptus MCP Server
"""

import pytest
from unittest.mock import Mock, patch
# Import the module to test
# import {target}


class Test{target.title()}:
    """Test cases for {target}"""

    def setup_method(self):
        """Set up test fixtures before each test method"""
        pass

    def teardown_method(self):
        """Clean up after each test method"""
        pass

    def test_initialization(self):
        """Test that {target} initializes correctly"""
        # Arrange
        # Act
        # Assert
        assert True  # Replace with actual test

    def test_main_functionality(self):
        """Test main functionality of {target}"""
        # Arrange
        # Act
        # Assert
        assert True  # Replace with actual test

    def test_error_handling(self):
        """Test error handling in {target}"""
        # Arrange
        # Act
        # Assert
        assert True  # Replace with actual test

    @pytest.mark.parametrize("input_data,expected", [
        # Add test cases here
        (None, None),
    ])
    def test_parameterized_cases(self, input_data, expected):
        """Test {target} with various inputs"""
        # Arrange
        # Act
        # Assert
        assert True  # Replace with actual test


if __name__ == "__main__":
    pytest.main([__file__])
'''
    else:  # unittest
        test_template = f'''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Test suite for {target}
Generated by Adaptus MCP Server
"""

import unittest
from unittest.mock import Mock, patch
# Import the module to test
# import {target}


class Test{target.title()}(unittest.TestCase):
    """Test cases for {target}"""

    def setUp(self):
        """Set up test fixtures before each test method"""
        pass

    def tearDown(self):
        """Clean up after each test method"""
        pass

    def test_initialization(self):
        """Test that {target} initializes correctly"""
        # Add your test code here
        pass

    def test_main_functionality(self):
        """Test main functionality of {target}"""
        # Add your test code here
        pass

    def test_error_handling(self):
        """Test error handling in {target}"""
        # Add your test code here
        pass


if __name__ == "__main__":
    unittest.main()
'''

    return {
        "template": test_template,
        "framework": framework,
        "instructions": f"""
        ## Test Implementation Instructions
        
        1. **Import Statements**: Uncomment and update import statements
        2. **Test Cases**: Replace placeholder assertions with actual test logic
        3. **Fixtures**: Update setup_method/setUp with necessary test data
        4. **Edge Cases**: Add tests for boundary conditions and error scenarios
        5. **Mock Dependencies**: Use Mock/patch to isolate the unit under test
        6. **Assertions**: Ensure each test has clear, meaningful assertions
        
        ## Best Practices for {framework.upper()}
        - Test naming: test_what_happens_when_condition
        - One assertion per test concept
        - Arrange-Act-Assert pattern
        - Test both happy path and error cases
        - Keep tests independent and isolated
        """,
        "next_steps": [
            "Save the template to test_{target.lower()}.py",
            "Add proper imports for your module",
            "Implement actual test logic",
            "Run with: "
            + ("pytest" if framework == "pytest" else "python -m unittest"),
        ],
    }


@mcp.tool()
def summarize(repo_path: str) -> dict[str, any]:
    """ホットスポットと難所を集約"""
    repo = Path(repo_path)
    if not repo.exists():
        return {"error": f"Repository path not found: {repo_path}"}

    # Find Python files
    python_files = list(repo.rglob("*.py"))
    if not python_files:
        return {"error": "No Python files found in repository"}

    # Analyze each file
    file_analysis = []
    total_loc = 0
    hotspots = []
    complexity_issues = []

    for file_path in python_files:
        try:
            code = _read_code(file_path)
            metrics = _basic_metrics(code)
            metrics["score"] = _score_formula(metrics)

            relative_path = str(file_path.relative_to(repo))
            file_analysis.append({
                "path": relative_path,
                "metrics": metrics,
            })

            total_loc += metrics["loc"]

            # Identify hotspots (high complexity or large files)
            if metrics["score"] > 5.0 or metrics["loc"] > 200:
                hotspots.append({
                    "path": relative_path,
                    "score": metrics["score"],
                    "loc": metrics["loc"],
                    "reason": (
                        "High debt score"
                        if metrics["score"] > 5.0
                        else "Large file size"
                    ),
                })

            # Identify complexity issues
            if metrics["branches"] > 20 or metrics["funcs"] > 15:
                complexity_issues.append({
                    "path": relative_path,
                    "branches": metrics["branches"],
                    "functions": metrics["funcs"],
                    "avg_args": metrics["avg_args"],
                })

        except Exception as e:
            file_analysis.append({
                "path": str(file_path.relative_to(repo)),
                "error": str(e),
            })

    # Sort by debt score
    hotspots.sort(key=lambda x: x["score"], reverse=True)

    # Generate recommendations
    recommendations = []
    if hotspots:
        recommendations.append(
            f"Focus on {len(hotspots)} hotspot files with highest debt scores"
        )
    if complexity_issues:
        recommendations.append(
            f"Consider refactoring {len(complexity_issues)} files with high complexity"
        )
    if total_loc > 1000:
        recommendations.append(
            "Large codebase detected - consider modularization"
        )

    return {
        "repository": repo_path,
        "summary": {
            "total_files": len(python_files),
            "total_loc": total_loc,
            "avg_loc_per_file": total_loc / len(python_files) if python_files else 0,
            "hotspots_count": len(hotspots),
            "complexity_issues_count": len(complexity_issues),
        },
        "hotspots": hotspots[:10],  # Top 10 hotspots
        "complexity_issues": complexity_issues[:10],  # Top 10 complexity issues
        "recommendations": recommendations,
        "file_analysis": file_analysis,
        "next_steps": [
            "Review hotspot files for refactoring opportunities",
            "Analyze complexity issues for design improvements",
            "Consider breaking down large files into smaller modules",
            "Implement automated testing for high-risk areas",
        ]
    }


@mcp.tool()
def propose_design(summary: str) -> dict[str, str]:
    """設計改善の雛形を返す"""
    mermaid = (
        "classDiagram\n"
        "  class UseCase {\n"
        "    +execute()\n"
        "  }\n"
        "  class DomainService {\n"
        "    +calc()\n"
        "  }\n"
        "  class Repository {\n"
        "    +find()\n"
        "  }\n"
        "  UseCase --> DomainService\n"
        "  UseCase --> Repository\n"
    )
    plan = "関心分離を徹底し、状態と手続を各ドメインへ再配置する。"
    return {"mermaid": mermaid, "plan": plan, "note": summary}


@mcp.tool()
def verify_patch(cmd_build: str = "", cmd_test: str = "") -> dict[str, int]:
    """外部検証を実行する"""
    rc_build = subprocess.call(cmd_build, shell=True) if cmd_build else 0
    rc_test = subprocess.call(cmd_test, shell=True) if cmd_test else 0
    return {"build_rc": rc_build, "test_rc": rc_test}


def main() -> None:
    """メインエントリーポイント"""
    mcp.run(transport="streamable-http")


if __name__ == "__main__":
    main()
